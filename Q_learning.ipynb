{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining grid word environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5\n",
    "goal_state = (4, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing  the Q table with zeros (5x5 grid, 4 actions)\n",
    "Q-values for (row, column, action) => Up, Down, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((grid_size, grid_size, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "alpha = Learnig rate, gamma = Discount Factor, epsilon = Exploration rate, episodes = Number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  \n",
    "gamma = 0.9  \n",
    "epsilon = 0.1  \n",
    "episodes = 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions: Up, Down, Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    if state == goal_state:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " next state after taking action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(state, action):\n",
    "    x, y = state\n",
    "    dx, dy = actions[action]\n",
    "    new_x, new_y = x + dx, y + dy\n",
    "    if 0 <= new_x < grid_size and 0 <= new_y < grid_size:\n",
    "        return (new_x, new_y)\n",
    "    return state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = (0, 0)          # start from the top-left corner\n",
    "    while state != goal_state:\n",
    "                            # epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(4)  # explore\n",
    "        else:\n",
    "            action = np.argmax(Q[state[0], state[1]])  # exploit\n",
    "        \n",
    "        next_state_value = next_state(state, action)\n",
    "        r = reward(next_state_value)  # get the reward\n",
    "        next_max = np.max(Q[next_state_value[0], next_state_value[1]])  # max Q-value for the next state\n",
    "        \n",
    "        \n",
    "        Q[state[0], state[1], action] = Q[state[0], state[1], action] + alpha * (r + gamma * next_max - Q[state[0], state[1], action])# Q-learning update rule\n",
    "        \n",
    "        \n",
    "        state = next_state_value# Move to the next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the Q-values and optimal path\n",
    "Create a heatmap for each action (Up, Down, Left, Right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for action_idx, ax in enumerate(axes):\n",
    "    \n",
    "    q_values_action = Q[:, :, action_idx]\n",
    "    sns.heatmap(q_values_action, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax, cbar=True, square=True)\n",
    "    ax.set_title(f\"Q-values for Action {['Up', 'Down', 'Left', 'Right'][action_idx]}\")\n",
    "    ax.set_xticks(np.arange(grid_size))\n",
    "    ax.set_yticks(np.arange(grid_size))\n",
    "    ax.set_xticklabels(np.arange(grid_size))\n",
    "    ax.set_yticklabels(np.arange(grid_size))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
